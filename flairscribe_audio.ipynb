{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pre-trained speaker diarization pipeline locally on our device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "c:\\Users\\Eric\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\inspect.py:1000: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  if ismodule(module) and hasattr(module, '__file__'):\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.0.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\Eric\\.cache\\torch\\pyannote\\models--pyannote--segmentation\\snapshots\\c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b\\pytorch_model.bin`\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.6.0+cpu. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf0d9ac3cad4e34969cdebe74f995fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hyperparams.yaml:   0%|          | 0.00/1.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Eric\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\speechbrain\\utils\\fetching.py:151: UserWarning: Using SYMLINK strategy on Windows for fetching potentially requires elevated privileges and is not recommended. See `LocalStrategy` documentation.\n",
      "  warnings.warn(\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "c:\\Users\\Eric\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\speechbrain\\utils\\autocast.py:68: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n",
      "c:\\Users\\Eric\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\speechbrain\\utils\\parameter_transfer.py:234: UserWarning: Requested Pretrainer collection using symlinks on Windows. This might not work; see `LocalStrategy` documentation. Consider unsetting `collect_in` in Pretrainer to avoid symlinking altogether.\n",
      "  warnings.warn(\n",
      "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "api_key = os.getenv(\"HUGGING_FACE_AUTH\")\n",
    "\n",
    "from pyannote.audio import Pipeline\n",
    "diarization_pipeline = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization@2.1\", use_auth_token=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the mp3 that we have created with 3 speakers in 4 tracks all concatenated together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "filename = \"combined_smaller\"\n",
    "combined_sample, combined_sample_rate = sf.read(f'combined/{filename}.wav')\n",
    "\n",
    "print(combined_sample_rate)\n",
    "print(combined_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can listen to the audio to see what it sounds like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "Audio(combined_sample, rate=combined_sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that pyannote.audio expects the audio input to be a PyTorch tensor of shape (channels, seq_len),\n",
    "so we need to perform this conversion prior to running the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "input_tensor = torch.from_numpy(combined_sample[None, :]).float()\n",
    "outputs = diarization_pipeline(\n",
    "    {\"waveform\": input_tensor, \"sample_rate\": combined_sample_rate}\n",
    ")\n",
    "\n",
    "\n",
    "# annotation_dict_list = list();\n",
    "# for segment in outputs.itersegments():\n",
    "#         annotation_dict_list.append({\n",
    "#                 \"segment\": segment,\n",
    "#                 \"track\": outputs.get_tracks(segment),\n",
    "#                 \"label\": outputs.get_labels(segment)\n",
    "#         })\n",
    "\n",
    "# outputs.for_json()[\"content\"]       \n",
    "# for annotation in annotation_dict_list:\n",
    "#         print(annotation)\n",
    "\n",
    "\n",
    "diarization_output = []\n",
    "for segment, track, label in outputs.itertracks(yield_label=True):\n",
    "    diarization_output.append({'segment': {'start': segment.start, 'end': segment.end},\n",
    "                        'track': track,\n",
    "                        'label': label})\n",
    "    \n",
    "for segment in diarization_output:\n",
    "    print(segment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Whisper model for our speech transcription system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "asr_pipeline = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-base\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the transcription for our sample audio, returning the segment level timestamps as well so that we know the start / end times for each segment.\n",
    "\n",
    "Whiper does not work well with audio files longer than 30 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_output = asr_pipeline(\n",
    "    combined_sample.copy(),\n",
    "    generate_kwargs={\"max_new_tokens\": 256},\n",
    "    return_timestamps=True,\n",
    ")\n",
    "\n",
    "print(asr_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the closest alignment between diarization and transcription timestamps by minimising the absolute distance between both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speechbox_trycatch_upto_idx import ASRDiarizationPipeline\n",
    "\n",
    "pipeline = ASRDiarizationPipeline(\n",
    "    asr_pipeline=asr_pipeline, diarization_pipeline=diarization_pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass the audio file to the composite pipeline and see what we get out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASRDiarizationPipeline calls both models again. The boxing part has been stripped out of ASRDiarizationPipeline into box.ipynb.\n",
    "# If hooking this up to the application will want to either use the box.ipynb method or use the below without calling the above.\n",
    "final_output = pipeline(combined_sample.copy())\n",
    "\n",
    "print(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "all_output = {\n",
    "    \"speakers\": final_output,\n",
    "    \"asr\": asr_output[\"chunks\"],\n",
    "    \"diarization\": diarization_output\n",
    "}\n",
    "\n",
    "with open(f\"output/{filename}.json\", \"w\") as f:\n",
    "    json.dump(all_output, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
